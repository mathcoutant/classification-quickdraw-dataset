{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76583b2",
   "metadata": {},
   "source": [
    "# Classification based on Quickdraw Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc18d27",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0789760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import cairocffi as cairo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ab56d",
   "metadata": {},
   "source": [
    "### Functions to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e370421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the classes that will be used\n",
    "def load_classes(file_path):\n",
    "    res = {}\n",
    "    count = 0\n",
    "    for line in open(file_path, 'r'):\n",
    "        res[count] = line.rstrip()\n",
    "        count+=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "516c92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The code is taken from the original GitHub of the QuickDrawDataset\n",
    "def vector_to_raster(vector_images, side=64, line_diameter=16, padding=16, bg_color=(0,0,0), fg_color=(1,1,1)):\n",
    "    \n",
    "    original_side = 256.\n",
    "    \n",
    "    surface = cairo.ImageSurface(cairo.FORMAT_ARGB32, side, side)\n",
    "    ctx = cairo.Context(surface)\n",
    "    ctx.set_antialias(cairo.ANTIALIAS_BEST)\n",
    "    ctx.set_line_cap(cairo.LINE_CAP_ROUND)\n",
    "    ctx.set_line_join(cairo.LINE_JOIN_ROUND)\n",
    "    ctx.set_line_width(line_diameter)\n",
    "\n",
    "    # scale to match the new size\n",
    "    # add padding at the edges for the line_diameter\n",
    "    # and add additional padding to account for antialiasing\n",
    "    total_padding = padding * 2. + line_diameter\n",
    "    new_scale = float(side) / float(original_side + total_padding)\n",
    "    ctx.scale(new_scale, new_scale)\n",
    "    ctx.translate(total_padding / 2., total_padding / 2.)\n",
    "\n",
    "    raster_images = []\n",
    "    for vector_image in vector_images:\n",
    "        # clear background\n",
    "        ctx.set_source_rgb(*bg_color)\n",
    "        ctx.paint()\n",
    "        \n",
    "        bbox = np.hstack(vector_image).max(axis=1)\n",
    "        offset = ((original_side, original_side) - bbox) / 2.\n",
    "        offset = offset.reshape(-1,1)\n",
    "        centered = [stroke + offset for stroke in vector_image]\n",
    "\n",
    "        # draw strokes, this is the most cpu-intensive part\n",
    "        ctx.set_source_rgb(*fg_color)        \n",
    "        for xv, yv in centered:\n",
    "            ctx.move_to(xv[0], yv[0])\n",
    "            for x, y in zip(xv, yv):\n",
    "                ctx.line_to(x, y)\n",
    "            ctx.stroke()\n",
    "\n",
    "        data = surface.get_data()\n",
    "        raster_image = np.copy(np.asarray(data)[::4])\n",
    "        raster_images.append(raster_image)\n",
    "    \n",
    "    return raster_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2332ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the data as an array containing images as 1D arrays\n",
    "def load_data(sample, n_images, dimension):\n",
    "    sample_data = [json.loads(line) for line in open(f'data/full_simplified_{sample}.ndjson', 'r')]\n",
    "    sample_data = random.sample(sample_data, k=n_images)\n",
    "    vector_images = [drawing_data['drawing'] for drawing_data in sample_data]\n",
    "    return np.array(vector_to_raster(vector_images, side=dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d906aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the datasets as images in the \"images\" folder\n",
    "def save_png(drawing_class, data, dimension):\n",
    "    if not os.path.exists(f\"images/{drawing_class}\"): \n",
    "        os.makedirs(f\"images/{drawing_class}\")\n",
    "\n",
    "    count = 0\n",
    "    for image_arr in data:\n",
    "        image_arr = np.reshape(image_arr, (dimension, -1))\n",
    "        img = Image.fromarray(image_arr, \"L\")\n",
    "        img.save(f\"images/{drawing_class}/{drawing_class}_{count}.png\")\n",
    "        count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead76647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(classes, n_images, img_dim, train_prop=0.8, save_images = False):\n",
    "  data_X = [] # To get the mean and standard deviation of the images\n",
    "  train_file_names = []\n",
    "  train_labels = []\n",
    "  test_file_names = []\n",
    "  test_labels = []\n",
    "  \n",
    "  for key, drawing_class in classes.items():\n",
    "      print(f\"Loading {drawing_class} data\")\n",
    "      data = load_data(drawing_class, n_images, dimension = img_dim)\n",
    "      data_X.append(data)\n",
    "      if(save_images):\n",
    "        save_png(drawing_class, data, img_dim)\n",
    "      file_names = [f\"images/{drawing_class}/{drawing_class}_{i}.png\" for i in range(len(data))]\n",
    "      labels = np.full(len(data), key)\n",
    "      train_file_names.append(file_names[:(int)(n_images*train_prop)])\n",
    "      train_labels.append(labels[:(int)(n_images*train_prop)])\n",
    "      test_file_names.append(file_names[(int)(n_images*train_prop):])\n",
    "      test_labels.append(labels[(int)(n_images*train_prop):])\n",
    "  \n",
    "  # Compute the mean and standard deviation of the images\n",
    "  data_X = np.array(data_X)\n",
    "  mean = np.mean(data_X)\n",
    "  std = np.std(data_X)\n",
    "\n",
    "  # Save the file names and labels\n",
    "  train_file_names = np.array(train_file_names).flatten()\n",
    "  json.dump(train_file_names.tolist(), open(\"train_file_names.json\", 'w'))\n",
    "  train_labels = np.array(train_labels).flatten()\n",
    "  json.dump(train_labels.tolist(), open(\"train_labels.json\", 'w'))\n",
    "  test_file_names = np.array(test_file_names).flatten()\n",
    "  json.dump(test_file_names.tolist(), open(\"test_file_names.json\", 'w'))\n",
    "  test_labels = np.array(test_labels).flatten()\n",
    "  json.dump(test_labels.tolist(), open(\"test_labels.json\", 'w'))\n",
    "  \n",
    "  return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba1702",
   "metadata": {},
   "source": [
    "### Dataset class that represent the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "555f9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, file_names, labels, transform=None):\n",
    "        self.file_names = file_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(\"images/axe/axe_0.png\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09cf0f",
   "metadata": {},
   "source": [
    "### Functions for the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5d8391a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3) # To increase maybe to 16\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3)# To increase maybe to 32\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(400, 128)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4483372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        print(output)\n",
    "        print(target)\n",
    "        loss = F.nll_loss(output, target[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15a9bd",
   "metadata": {},
   "source": [
    "### Main part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab57ce",
   "metadata": {},
   "source": [
    "Variables describing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1bcd9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images will be 64x64\n",
    "img_dim = 28\n",
    "# Number of images taken for each animal\n",
    "n_images = 1000\n",
    "# Proportion used for to train the model\n",
    "train_prop = 0.8\n",
    "learning_rate = 0.001\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e76da2",
   "metadata": {},
   "source": [
    "Store the classes in a dictionnary with their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7a44a4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'axe', 1: 'bicycle'}\n"
     ]
    }
   ],
   "source": [
    "classes = load_classes(\"class_names.txt\")\n",
    "n_classes = len(classes)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f8a54",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The preprocess function do multiple thingd : <br>\n",
    "- Build the png images from the json files. Like this the PyTorch Dataset object will search the images directly in the files\n",
    "- Separate the dataset in train and test datasets by storing the names of the \n",
    "- Process the mean and the standard deviation and return it to later normalize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "749b0f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading axe data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bicycle data\n"
     ]
    }
   ],
   "source": [
    "mean, std = preprocess_data(classes, n_images=n_images, img_dim=img_dim, train_prop=train_prop, save_images = False)\n",
    "\n",
    "train_files = json.load(open(\"train_file_names.json\", 'r'))\n",
    "test_files = json.load(open(\"test_file_names.json\", 'r'))\n",
    "\n",
    "temp_train_labels = json.load(open(\"train_labels.json\", 'r'))\n",
    "train_labels = []\n",
    "for label in temp_train_labels:\n",
    "    a = np.zeros((n_classes,), dtype=int); \n",
    "    a[label] = 1 \n",
    "    train_labels.append(a)\n",
    "\n",
    "temp_test_labels = json.load(open(\"test_labels.json\", 'r'))\n",
    "test_labels = []\n",
    "for label in temp_test_labels:\n",
    "    a = np.zeros((n_classes,), dtype=int); \n",
    "    a[label] = 1 \n",
    "    test_labels.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb54f4b",
   "metadata": {},
   "source": [
    "Create the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cc81e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "train_dataset = QuickDrawDataset(train_files, train_labels, transform=preprocess)\n",
    "test_dataset = QuickDrawDataset(test_files, test_labels, transform=preprocess)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "752b7a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"images/axe/axe_0.png\")\n",
    "image = preprocess(image)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7ec90670",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x25600 and 400x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m log_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[136], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch, log_interval)\u001b[0m\n\u001b[0;32m      4\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(target)\n",
      "File \u001b[1;32mc:\\Users\\Mathieu\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mathieu\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[124], line 18\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m---> 18\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(x)\n",
      "File \u001b[1;32mc:\\Users\\Mathieu\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mathieu\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mathieu\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x25600 and 400x128)"
     ]
    }
   ],
   "source": [
    "model = Net(n_classes)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "log_interval = 10\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00552653",
   "metadata": {},
   "source": [
    "For the datas :\n",
    "- sort to keep only the recognized drawings\n",
    "\n",
    "For the CNN:\n",
    "- (batch normailzation)\n",
    "- conv relu pooling conv relu pooling dropout\n",
    "\n",
    "For vizualization and documentation:\n",
    "- Confusion Matrix\n",
    "- evolution of the error with epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5184791",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bd913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
